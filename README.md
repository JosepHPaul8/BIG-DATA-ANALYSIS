# BIG-DATA-ANALYSIS

*COMPANY NAME* : CODTECH IT SOLUTIONS

*NAME* : Joseph Paul D

*INTERN ID*: CT06WZ39

*DOMAIN* : Data Analytics

*DURATION* : 6 weeks

*MENTOR* : Neela Santhosh

*DESCRIPTION*

This Jupyter notebook explores big data analysis using PySpark, a powerful framework for handling and processing large datasets. The notebook begins by setting up the PySpark environment and creating a SparkSession, which serves as the entry point for working with structured data. The dataset under analysis—presumably large and complex—is loaded using spark.read.csv() with options to infer schema and handle headers, showcasing PySpark’s ability to efficiently read massive CSV files.

After loading the data, the notebook walks through a series of exploratory data analysis (EDA) operations using PySpark DataFrame methods. This includes displaying the schema, viewing sample records, counting rows, and generating summary statistics such as mean, max, min, and standard deviation. It also performs column-based operations like filtering, grouping, and aggregations—key for identifying trends and patterns in big data.

The notebook demonstrates the use of PySpark’s groupBy() and agg() functions for data summarization, along with sorting and data cleaning techniques. These operations help derive business insights, such as high-performing product segments or anomalies in data.

Overall, this notebook serves as a practical example of how to apply PySpark for scalable big data processing, making it ideal for analytics tasks that exceed the capacity of traditional pandas-based workflows.

*OUTPUT*

![Image](https://github.com/user-attachments/assets/b9933c3e-d484-4c56-ada8-eb21722a7f84)
